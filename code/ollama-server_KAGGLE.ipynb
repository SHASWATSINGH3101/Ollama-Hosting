{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-02T17:21:13.536535Z",
     "iopub.status.busy": "2024-12-02T17:21:13.535748Z",
     "iopub.status.idle": "2024-12-02T17:21:29.925457Z",
     "shell.execute_reply": "2024-12-02T17:21:29.924500Z",
     "shell.execute_reply.started": "2024-12-02T17:21:13.536490Z"
    },
    "scrolled": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Download Ollama\n",
    "!curl https://ollama.ai/install.sh | sh\n",
    "# Install required libraries\n",
    "!pip install aiohttp pyngrok\n",
    "\n",
    "import os\n",
    "import asyncio\n",
    "\n",
    "# Update LD_LIBRARY_PATH to include NVIDIA library directories\n",
    "os.environ.update({\n",
    "    'LD_LIBRARY_PATH': '/usr/local/nvidia/lib64:/usr/local/cuda-12.3/lib64:/usr/local/cuda-12.3/compat/libcuda.so'\n",
    "})\n",
    "\n",
    "async def run_process(cmd):\n",
    "    print('>>> starting', *cmd)\n",
    "    p = await asyncio.subprocess.create_subprocess_exec(\n",
    "        *cmd,\n",
    "        stdout=asyncio.subprocess.PIPE,\n",
    "        stderr=asyncio.subprocess.PIPE,\n",
    "    )\n",
    "\n",
    "    async def pipe(lines):\n",
    "        async for line in lines:\n",
    "            print(line.strip().decode('utf-8'))\n",
    "\n",
    "    await asyncio.gather(\n",
    "        pipe(p.stdout),\n",
    "        pipe(p.stderr),\n",
    "    )\n",
    "\n",
    "# Register an account at ngrok.com and create an authtoken, and place it here\n",
    "await asyncio.gather(\n",
    "    run_process(['ngrok', 'config', 'add-authtoken', 'your-authtoken'])\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-02T17:21:29.927763Z",
     "iopub.status.busy": "2024-12-02T17:21:29.927295Z",
     "iopub.status.idle": "2024-12-02T17:26:06.708554Z",
     "shell.execute_reply": "2024-12-02T17:26:06.707298Z",
     "shell.execute_reply.started": "2024-12-02T17:21:29.927714Z"
    },
    "scrolled": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Start ollama and ngrok\n",
    "await asyncio.gather(\n",
    "    run_process(['ollama', 'serve']),\n",
    "    run_process(['ngrok', 'http', '--log', 'stderr', '11434', '--host-header', 'localhost:11434'])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model uploaing ðŸ‘‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-02T17:26:22.291438Z",
     "iopub.status.busy": "2024-12-02T17:26:22.290747Z",
     "iopub.status.idle": "2024-12-02T17:27:00.123408Z",
     "shell.execute_reply": "2024-12-02T17:27:00.122345Z",
     "shell.execute_reply.started": "2024-12-02T17:26:22.291401Z"
    },
    "scrolled": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Step 1: List the contents of the Ollama models directory\n",
    "import os\n",
    "print(\"Listing the contents of /root/.ollama/models/\")\n",
    "print(os.listdir(\"/root/.ollama/models/\"))\n",
    "\n",
    "# Step 2: Create the target directory on Kaggle's working environment\n",
    "print(\"Creating the directory /kaggle/working/ollama_models/\")\n",
    "os.makedirs(\"/kaggle/working/ollama_models\", exist_ok=True)\n",
    "\n",
    "# Step 3: Copy the contents from the Ollama model directory to the target directory\n",
    "import shutil\n",
    "print(\"Copying models to /kaggle/working/ollama_models/\")\n",
    "shutil.copytree(\"/root/.ollama/models\", \"/kaggle/working/ollama_models\", dirs_exist_ok=True)\n",
    "\n",
    "# Step 4: Install Kaggle Hub\n",
    "print(\"Installing kagglehub package\")\n",
    "!pip install kagglehub\n",
    "\n",
    "# Step 5: Set Kaggle username and key for authentication\n",
    "import os\n",
    "os.environ[\"KAGGLE_USERNAME\"] = \"\"  # Replace with your actual Kaggle username\n",
    "os.environ[\"KAGGLE_KEY\"] = \"\"  # Replace with your actual Kaggle API key\n",
    "\n",
    "# Step 6: Upload model to Kaggle Hub\n",
    "import kagglehub\n",
    "\n",
    "# Set your model handle and local model directory path\n",
    "handle = \"KAGGLE_USERNAME/model-name/package/version\"  # Replace with your model handle\n",
    "local_model_dir = \"/kaggle/working/ollama_models/\"\n",
    "\n",
    "# Upload the model with optional version notes\n",
    "kagglehub.model_upload(handle, local_model_dir, version_notes=\"Upload date :- 02/12/2024\")\n",
    "\n",
    "# Optional: You can also specify a license\n",
    "# kagglehub.model_upload(handle, local_model_dir, license_name=\"Apache 2.0\")\n",
    "\n",
    "# Optional: Specify patterns to ignore during upload\n",
    "# kagglehub.model_upload(handle, local_model_dir, ignore_patterns=[\"original/\", \"*.tmp\"])\n",
    "\n",
    "print(\"Model upload complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Soft-link model ðŸ‘‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-02T17:00:22.244996Z",
     "iopub.status.busy": "2024-12-02T17:00:22.244656Z",
     "iopub.status.idle": "2024-12-02T17:00:35.268795Z",
     "shell.execute_reply": "2024-12-02T17:00:35.267889Z",
     "shell.execute_reply.started": "2024-12-02T17:00:22.244967Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "def sync_or_copy(source_dir, target_dir, folder_name):\n",
    "    \"\"\"\n",
    "    Handle syncing or copying a folder based on its existence in the target directory.\n",
    "    \n",
    "    If the filesystem is read-only, it will skip the write operation.\n",
    "    \n",
    "    Parameters:\n",
    "    - source_dir (str): Path to the source directory containing the folder to copy.\n",
    "    - target_dir (str): Path to the target directory where the folder should be copied or synced.\n",
    "    - folder_name (str): Name of the folder to process (e.g., 'manifests', 'blobs').\n",
    "    \"\"\"\n",
    "    source_folder = os.path.join(source_dir, folder_name)\n",
    "    target_folder = os.path.join(target_dir, folder_name)\n",
    "\n",
    "    if os.path.exists(target_folder):\n",
    "        print(f\"'{folder_name}' folder exists at {target_folder}. Copying contents...\")\n",
    "        for item in os.listdir(source_folder):\n",
    "            src_item = os.path.join(source_folder, item)\n",
    "            dest_item = os.path.join(target_folder, item)\n",
    "            \n",
    "            if os.path.isdir(src_item):\n",
    "                if os.path.exists(dest_item):\n",
    "                    print(f\"Directory exists, merging: {dest_item}\")\n",
    "                try:\n",
    "                    shutil.copytree(src_item, dest_item, dirs_exist_ok=True)\n",
    "                except OSError as e:\n",
    "                    print(f\"Skipping {src_item}: {e}\")\n",
    "            else:\n",
    "                try:\n",
    "                    shutil.copy2(src_item, dest_item)\n",
    "                except OSError as e:\n",
    "                    print(f\"Skipping {src_item}: {e}\")\n",
    "    else:\n",
    "        print(f\"'{folder_name}' folder does not exist at {target_dir}. Copying the entire folder...\")\n",
    "        try:\n",
    "            shutil.copytree(source_folder, target_folder)\n",
    "        except OSError as e:\n",
    "            print(f\"Error copying {folder_name}: {e}\")\n",
    "\n",
    "# Define source and target paths\n",
    "source_dir = \"/kaggle/input/lama3_2_1b/pytorch/v1/3\"\n",
    "target_dir = \"/root/.ollama/models\"\n",
    "\n",
    "# Process the manifests and blobs folders\n",
    "sync_or_copy(source_dir, target_dir, \"manifests\")\n",
    "sync_or_copy(source_dir, target_dir, \"blobs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "isSourceIdPinned": true,
     "modelId": 180038,
     "modelInstanceId": 157626,
     "sourceId": 184905,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 180918,
     "modelInstanceId": 158552,
     "sourceId": 185968,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
